{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, email and UFID.\n",
    "Please do not modify instruction cells or any cells with automated tests (marked with `[ASSERTS]`). Note: you can add new cells if you need them, but answers must be in the cells with `YOUR CODE HERE` or \"YOUR ANSWER HERE\" comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef2236604b2cde6702781a68b6c36de1",
     "grade": false,
     "grade_id": "homework-preamble",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Homework 3: Gradient Descent & Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5658c9a4ed21f362746fdaeb7fb21351",
     "grade": false,
     "grade_id": "preamble-name",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preamble: Write your Name, Email and UFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50438064f8ce6f02be7713a7f7c1a39c",
     "grade": false,
     "grade_id": "name-email-ufid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homework 3 -- name: Tre' Jeter, email: t.jeter@ufl.edu, UFID: 19469876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NAME = 'Tre\\' Jeter'\n",
    "EMAIL = 't.jeter@ufl.edu'\n",
    "UFID = 19469876\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "print('Homework 3 -- name: {}, email: {}, UFID: {}\\n'.format(NAME, EMAIL, UFID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41a13a5f6e2fa1b47621d28e926c2cf",
     "grade": true,
     "grade_id": "name-email-ufid-asserts",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that your name, email, and UFID is filled in.\"\"\"\n",
    "assert NAME != '' and NAME != 'Your name here.' and len(NAME) > 3\n",
    "assert EMAIL != '' and EMAIL != 'Your email here.' and len(EMAIL) > 7\n",
    "assert type(UFID) == int and UFID != 12345678 and UFID >= 10000000 and UFID <= 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b4f30c4bbc2f9927b1410eeb1f001b3",
     "grade": false,
     "grade_id": "preamble-academic-integrity",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Academic Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99184dabc791053131230787b9f498b8",
     "grade": false,
     "grade_id": "preamble-academic-integrity-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### <span style=\"color:red;\">This is an individual assignment. Academic integrity violations (i.e., cheating, plagiarism) will be reported to SCCR!</span><br/>\n",
    "#### The official CISE policy recommended for such offenses is a course grade of E. Additional sanctions may be imposed by SCCR such as marks on your permanent educational transcripts, dismissal or expulsion.\n",
    "#### Reminder of the Honor Pledge: On all work submitted for credit by Students at the University of Florida, the following pledge is either required or implied: *\"On my honor, I have neither given nor received unauthorized aid in doing this assignment.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d8d2452e1d9f6d547eae6447b7ca369",
     "grade": false,
     "grade_id": "cell-preamble-academic-integrity-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Acknowledgement: Do you acknowledge and understand the academic integrity warning above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89bc9ed2e09cb9069b92dc24a3bc081a",
     "grade": false,
     "grade_id": "academic-integrity",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "academic_integrity_acknowledgement = True\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d6eb103ab3a60e964c163468d9aa7a",
     "grade": true,
     "grade_id": "academic-integrity-assert",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that you acknowledge the academic integrity warning, you understand it and have been reminded of the UF Honor Pledge.\"\"\"\n",
    "assert academic_integrity_acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1133d37a01d4455577d6a6408fe4bcef",
     "grade": false,
     "grade_id": "task1-instructb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will use the Bike Sharing dataset (hourly). A version of this dataset is included in the homework handout archive.\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike.\n",
    "### In this task you will load the data and preprocess it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40e0c6343e3d6ae8ef5568125e9de64f",
     "grade": false,
     "grade_id": "task1-instructc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The following cell's code (import statements etc.) is provided for you and you should not need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119d3a62a34e7425f288493659994237",
     "grade": false,
     "grade_id": "task1-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "### Python version: 3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]\n",
      "### NumPy version: 1.20.3\n",
      "### Scikit-learn version: 1.4.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('------------')\n",
    "\n",
    "def var_exists(var_name):\n",
    "    return (var_name in globals() or var_name in locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc6d3b909b447a027649ca1d83883d9",
     "grade": false,
     "grade_id": "seed_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This is the seed we will use, do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bba71034a59520e609f24b40475e074",
     "grade": false,
     "grade_id": "setting_seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [16, 2, 2] # proportions for train - val - test splits\n",
    "\n",
    "epsf = 1e-9 # small epsilon value for floating point comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d746a78509c270bb1f51eff6a455a1f6",
     "grade": true,
     "grade_id": "seed_checking",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check seed. \"\"\"\n",
    "assert seed == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26690ee65de9a82553bc05a151c027e",
     "grade": false,
     "grade_id": "task1-instructd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loading data (set the path correctly so it runs on your machine --- don't submit the data file with your notebook).\n",
    "#### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "424a20cc93e7cd774f4c8c7ca4e10b8e",
     "grade": false,
     "grade_id": "task1-loaddata",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in the path to the directory where 'bikesharehour.csv.gz' is located.\n",
    "\"\"\"\n",
    "data_root = './data' #put the path here\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f165779998390d2ddff4aea0e0ac8453",
     "grade": true,
     "grade_id": "task1-loaddata-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      16320 non-null  float64\n",
      " 1   year        16231 non-null  float64\n",
      " 2   month       16304 non-null  float64\n",
      " 3   hour        16254 non-null  float64\n",
      " 4   holiday     16277 non-null  float64\n",
      " 5   weekday     16282 non-null  float64\n",
      " 6   workingday  16297 non-null  float64\n",
      " 7   weathersit  16324 non-null  float64\n",
      " 8   temp        16242 non-null  float64\n",
      " 9   atemp       16271 non-null  float64\n",
      " 10  hum         16252 non-null  float64\n",
      " 11  windspeed   16281 non-null  float64\n",
      " 12  registered  16244 non-null  float64\n",
      " 13  nsqrtc      16263 non-null  float64\n",
      " 14  count       17379 non-null  int64  \n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_fp = os.path.join(data_root, 'bikesharehour.csv.gz')\n",
    "assert os.path.exists(dataset_fp), 'Dataset not found ({})!'.format(dataset_fp)\n",
    "df = pd.read_csv(dataset_fp, compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f7d9c6b0baab5be1a1e13a4030c3ed7",
     "grade": false,
     "grade_id": "cell-c33496a06e653c69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following code will pre-process the dataset (no need to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17f4100a571c28e9a0c26cf95b33f3fb",
     "grade": false,
     "grade_id": "task1-preproc-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['season', 'year', 'month', 'hour', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered', 'nsqrtc'] --- target: count\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# grabing stuff and pre-processing \n",
    "all_xy = df.to_numpy() # grab all the data as a numpy matrix\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]\n",
    "print('features: {} --- target: {}'.format(features, target))\n",
    "\n",
    "# split into x and y\n",
    "all_x_nan = all_xy[:,:-1]\n",
    "all_y = all_xy[:,-1]\n",
    "\n",
    "# rescale y to be within [0,100]\n",
    "y_scale = 100.0\n",
    "min_y = np.amin(all_y)\n",
    "max_y = np.amax(all_y)\n",
    "all_y = y_scale * (all_y - min_y)/(max_y - min_y)\n",
    "\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy='median', copy=True)\n",
    "\n",
    "all_x_mf = mf_imputer.fit_transform(all_x_nan)\n",
    "all_x = all_x_mf\n",
    "\n",
    "# check that the shape is correct\n",
    "assert all_x.shape == (17379, 14)\n",
    "\n",
    "# check that there are no more NaNs\n",
    "assert np.sum(np.sum(np.isnan(all_x), axis=0)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f3de30be55417787a429abc04da21a6",
     "grade": false,
     "grade_id": "cell-8c4bf6378cc5db72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will create two splits of the data in train-test-val one with standardized features the other without (unscaled features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19d1adae0627ecd1cb0d475184c6d81",
     "grade": false,
     "grade_id": "task1-preproc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# unscaled --- split the data into train, test, val\n",
    "train_prop = 1.0 - prop_vec[0] / np.sum(prop_vec)\n",
    "train_x_unscaled, tmp_x_unscaled, train_y_unscaled, tmp_y_unscaled = train_test_split(all_x, all_y, test_size=train_prop, random_state=seed)\n",
    "\n",
    "val_prop = 1.0 - prop_vec[1] / np.sum(prop_vec[1:])\n",
    "val_x_unscaled, test_x_unscaled, val_y_unscaled, test_y_unscaled = train_test_split(tmp_x_unscaled, tmp_y_unscaled, test_size=val_prop, random_state=seed)\n",
    "\n",
    "\n",
    "# rescaled features (default)\n",
    "scaler = StandardScaler(copy=True) # z-score normalize the features\n",
    "scaled_all_x = scaler.fit_transform(all_x) \n",
    "\n",
    "# split the data into train, test, val\n",
    "train_prop = 1.0 - prop_vec[0] / np.sum(prop_vec)\n",
    "train_x, tmp_x, train_y, tmp_y = train_test_split(scaled_all_x, all_y, test_size=train_prop, random_state=seed)\n",
    "\n",
    "val_prop = 1.0 - prop_vec[1] / np.sum(prop_vec[1:])\n",
    "val_x, test_x, val_y, test_y = train_test_split(tmp_x, tmp_y, test_size=val_prop, random_state=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape\n",
    "assert train_x.shape == (13903, 14) and train_y.shape == (13903,) and test_x.shape == (1738, 14) and test_y.shape == (1738,)\n",
    "\n",
    "assert train_x.shape == train_x_unscaled.shape and val_x.shape == val_x_unscaled.shape and np.all(np.abs(train_y - train_y_unscaled) <= 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "684d26618d91b9a12ed158436c1b1487",
     "grade": false,
     "grade_id": "task1-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 1] (35 points) Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "563450c152dcbabbf00c2f1f42993637",
     "grade": false,
     "grade_id": "cell-5b80277d217a1a0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's train a linear regression model that we can use as a point of comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea3374be767e0f5beaa793d91ebd2f73",
     "grade": false,
     "grade_id": "task1-lr-comp",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lrmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4fa64440e34db55cbca5aa00adbfadd",
     "grade": false,
     "grade_id": "task1-instructa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1] Over the next few cells you will fill in code to implement what we need for a training loop with gradient descent. \n",
    "### The version of gradient descent we will implement is fairly generic in that it can capture batch gradient descent, stochastic gradient descent, and mini-batch SGD.  However, as a result of the flexibility it offers it has many components which you have to implement correctly. So you should read the provided code skeleton and think carefully of how the entire training loop works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2343921bd7cb4712be704989ec2bbf37",
     "grade": false,
     "grade_id": "task1a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1a] (5 points) Fill in the code for the function predict_linear(). It takes a vector of parameters theta and a feature matrix X. The prediction is computed as w x + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "497598b693a09ad3f1eef6d735e82bf0",
     "grade": false,
     "grade_id": "task1a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code in the function (~1 line)\n",
    "\"\"\"\n",
    "# given model parameters 'theta' and a feature matrix 'X', return predictions (for a linear model)\n",
    "def predict_linear(theta, X):\n",
    "    b = theta[0]\n",
    "    w = theta[1:]\n",
    "    \n",
    "    assert w.shape[0] == X.shape[1]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    pred_y = np.dot(X, w) + b\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    assert pred_y.shape == (X.shape[0],)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f7cf66e39b77555a63db97f253ea94f",
     "grade": true,
     "grade_id": "task1a-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1a completed. \"\"\"\n",
    "\n",
    "assert var_exists('predict_linear')\n",
    "tmp_X_ = np.random.randint(low=0, high=10, size=(7,4))\n",
    "tmp_theta_ = np.random.uniform(size=(5,))\n",
    "out = predict_linear(tmp_theta_, tmp_X_)\n",
    "\n",
    "assert out.shape == (7,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6c3d349018b433735ea86ccfbb3bfca",
     "grade": false,
     "grade_id": "cell-54650b2befcaa808",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following code is provided and we will use to evaluate the models we train. (You do not need to modify it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e5bf300789ec99a4222310528a5bce",
     "grade": false,
     "grade_id": "task1-providedb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def r2_mse_mae_eval(model_theta, tr_x, tr_y, v_x, v_y, predict_fn=predict_linear, pref='', verb=True):\n",
    "    # predictions\n",
    "    train_pred = predict_fn(model_theta, tr_x)\n",
    "    val_pred = predict_fn(model_theta, v_x)\n",
    "    \n",
    "    # R^2 the coefficient of determination\n",
    "    train_r2 = r2_score(tr_y, train_pred)\n",
    "    val_r2 = r2_score(v_y, val_pred)\n",
    "    \n",
    "    if verb:\n",
    "        print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, train_r2, val_r2))\n",
    "\n",
    "    # measure the error (MSE) wrt true target\n",
    "    train_mse = mean_squared_error(tr_y, train_pred)\n",
    "    val_mse = mean_squared_error(v_y, val_pred)\n",
    "    if verb:\n",
    "        print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_mse, val_mse))\n",
    "\n",
    "    train_mae = mean_absolute_error(tr_y, train_pred)\n",
    "    val_mae = mean_absolute_error(v_y, val_pred)\n",
    "\n",
    "    if verb:\n",
    "        print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_mae, val_mae))\n",
    "\n",
    "    return train_r2, val_r2, train_mse, val_mse, train_mae, val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f42e96a6fb6c64b1068790c42b4d5ae5",
     "grade": false,
     "grade_id": "task1b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1b] (10 points) Fill in the code for generic_partition_shuffle_sgd(). Read the code of the function carefully and try to understand how it works before you fill in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a71b032a71d4b3dcb5288a3b2d075ea4",
     "grade": false,
     "grade_id": "task1b-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The function below takes training data (X, y) and performs gradient descent for 'num_epochs' using batches of size 'batch_size'. Most important are the following arguments:\n",
    "#### - model_fns_tup is a tuple consisting of three functions for prediction, evaluating the loss, and computing the gradients.\n",
    "#### - init_params_fn is a function to initialize the parameter vector theta\n",
    "#### - lr_schedule_fn is a function to calculate the learning rate based on the epoch (learning rate schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b291271784ee6c2b97ba5427960e7fa4",
     "grade": false,
     "grade_id": "task1b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code in the function (~3-4 lines). Your code should grab each batch as (mb_X, mb_y) based on the shuffled data. \n",
    "You can ignore the part about clipping gradients for now (will be revisited in a later task).\n",
    "\"\"\"\n",
    "def generic_partition_shuffle_sgd(X, y, model_fns_tup, init_params_fn, lr_schedule_fn, num_params=None, val_data=None, clip_gradient=False, \n",
    "                                  num_epochs=1000, batch_size=100, stop_tol=1e-10, verbose=False, verb_freq=0):\n",
    "    (n, m) = X.shape\n",
    "    assert n == y.shape[0]\n",
    "\n",
    "    if verb_freq < 1:\n",
    "        verb_freq = int(np.ceil(num_epochs/5))\n",
    "\n",
    "    # enforce meaningful batch size\n",
    "    assert batch_size == int(batch_size)\n",
    "    if batch_size < 1:\n",
    "        batch_size = 1\n",
    "    elif batch_size > n:\n",
    "        batch_size = n\n",
    "\n",
    "    if num_params == None:\n",
    "        num_params = m\n",
    "        \n",
    "    theta = init_params_fn(num_params) # initialize the model parameters\n",
    "\n",
    "    # check the functions\n",
    "    assert model_fns_tup != None and type(model_fns_tup) == tuple and len(model_fns_tup) == 3\n",
    "    predict_fn, loss_fn, gradient_fn = model_fns_tup\n",
    "\n",
    "    assert lr_schedule_fn != None # learning schedule\n",
    "\n",
    "    if val_data != None:\n",
    "        val_X, val_y = val_data\n",
    "\n",
    "    num_batches = int(np.ceil(n/batch_size))\n",
    "\n",
    "    train_loss = np.zeros((num_epochs, num_batches))\n",
    "    val_loss = np.zeros((num_epochs,))\n",
    "\n",
    "    loss_ma = None\n",
    "    for epoch in range(0, num_epochs):           \n",
    "        prev_theta = theta\n",
    "\n",
    "        # shuffle the data\n",
    "        pi = np.random.permutation(n)\n",
    "        shuf_X = X[pi,:]\n",
    "        shuf_y = y[pi]\n",
    "\n",
    "        # compute learning rate\n",
    "        eta = lr_schedule_fn(epoch) \n",
    "        assert eta > 0, 'Learning rate must be positive.'\n",
    "        \n",
    "        # inner loop over batches\n",
    "        for batch_start in range(0, n, batch_size):\n",
    "            \n",
    "            batch_idx = batch_start // batch_size\n",
    "            iteration_idx = epoch * num_batches + batch_idx\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            mb_X = shuf_X[batch_start:batch_start+batch_size]\n",
    "            mb_y = shuf_y[batch_start:batch_start+batch_size]\n",
    "            # raise NotImplementedError()\n",
    "            assert mb_X.shape[0] == mb_y.shape[0]\n",
    "            \n",
    "            gradient = gradient_fn(theta, mb_X, mb_y) # calculate the gradient vector\n",
    "            assert gradient.shape == theta.shape  \n",
    "\n",
    "            if clip_gradient:\n",
    "                # L2-norm clip the gradient\n",
    "                # YOUR CODE HERE\n",
    "                norm_grad = np.linalg.norm(gradient)\n",
    "                max_norm_grad = 1.0\n",
    "                if norm_grad > max_norm_grad:\n",
    "                    gradient *= max_norm_grad / norm_grad\n",
    "                # raise NotImplementedError()\n",
    "\n",
    "            # update theta (this is the actual gradient descent step)\n",
    "            theta = theta - eta * gradient\n",
    "\n",
    "            # compute the loss(es)\n",
    "            train_lossval = loss_fn(theta, mb_X, mb_y)\n",
    "            train_loss[epoch, batch_idx] = train_lossval\n",
    "\n",
    "            # compute diff in theta from previous iteration\n",
    "            diff = theta - prev_theta\n",
    "            l2ndiff = np.linalg.norm(diff)\n",
    "\n",
    "            if verbose and np.mod(iteration_idx, verb_freq) == 0:\n",
    "                loss_ma = 0.5 * loss_ma + 0.5 * train_lossval if loss_ma is not None else train_lossval\n",
    "                print('[Iter {} (epoch {})] train loss: {:.2f}, lr: {:.9f}, theta diff (l2-norm): {:.9f}.'.format(iteration_idx, epoch, loss_ma, eta, l2ndiff))\n",
    "\n",
    "        # validation loss, once per epoch\n",
    "        val_lossval = loss_fn(theta, val_X, val_y) if val_data != None else None\n",
    "        if val_lossval != None:\n",
    "            val_loss[epoch] = val_lossval\n",
    "\n",
    "        \n",
    "        if l2ndiff < stop_tol: # do we stop?\n",
    "            if verbose:\n",
    "                print('Stop condition reached (epoch: {} -- l2ndiff: {}).'.format(epoch, l2ndiff))\n",
    "            break\n",
    "            \n",
    "    train_loss = np.array(train_loss)\n",
    "    val_loss = np.array(val_loss) if len(val_loss) > 0 else None\n",
    "    ret_dc = {'theta': theta.reshape(-1,), 'train_loss': train_loss, 'val_loss': val_loss, 'last_epoch': epoch, 'num_batches': num_batches, 'l2ndiff': l2ndiff, 'mb_X_shape': mb_X.shape, 'gradient': gradient}\n",
    "    \n",
    "    return ret_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9afb7af2cdaf8e0b8e6b17aa0978b905",
     "grade": true,
     "grade_id": "task1b-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1b completed. \"\"\"\n",
    "\n",
    "assert var_exists('generic_partition_shuffle_sgd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "480be68362819af01a7208be5dda266c",
     "grade": false,
     "grade_id": "task1c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1c] (10 points) Fill in the code for the following functions for parameter initialization and learning rate scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cdd07a4c1d6286373a852993c4faff7",
     "grade": false,
     "grade_id": "task1c-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### - constant_lr_schedule() should always return the learning rate eta.\n",
    "#### - random_uniform_init() should return a parameter vector of shape (m,1) uniformly random between low and high.\n",
    "#### - zero_init() should return a parameter vector of shape (m,1) all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d1875014a24194ea545ec0f5a99bf33",
     "grade": false,
     "grade_id": "task1c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code below (~1 line for each function).\n",
    "\"\"\"\n",
    "\n",
    "# this learning rate schedule is provided as an example\n",
    "def log1p_decay_lr_schedule(eta, epoch):\n",
    "    return eta / (1.0 + np.log1p(epoch))\n",
    "\n",
    "# this is a constant schedule, it should always return the learning rate eta (regardless of epoch)\n",
    "def constant_lr_schedule(eta, epoch):\n",
    "    # YOUR CODE HERE\n",
    "    return eta\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "def random_uniform_init(m, low=-1.0, high=1.0):\n",
    "    # YOUR CODE HERE\n",
    "    return np.random.uniform(low=low, high=high, size=(m, 1))\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "def zero_init(m):\n",
    "    # YOUR CODE HERE\n",
    "    return np.zeros((m, 1))\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c56fc68d547ec748a7132c968e3a3a3f",
     "grade": true,
     "grade_id": "task1c-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1c completed. \"\"\"\n",
    "\n",
    "assert var_exists('constant_lr_schedule') and var_exists('random_uniform_init') and var_exists('zero_init')\n",
    "\n",
    "assert constant_lr_schedule(10, 0) == constant_lr_schedule(10, 3) and constant_lr_schedule(10, 17) == 10\n",
    "assert np.all(np.abs(zero_init(10)) < epsf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "872321dd0c19457b731366c1c0ca9690",
     "grade": false,
     "grade_id": "task1d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1d] (10 points) Now let's implement the loss function and gradient computation function. Fill in the code below.\n",
    "### - loss_mse() needs to calculate the MSE loss value on (X,y) given the linear model described by 'theta'.\n",
    "### - gradient_mse() needs to compute the gradient of the MSE loss with respect to the parameters 'theta'. You may want to look at the course slides to help you figure out the gradient.\n",
    "### Note: while it may be tempting to use predict_linear() (and it is possible to get this to work with some effort) the code is set up to include a constant feature for the bias so the loss and gradients are easier to compute. So it will be easier to avoid predict_linear() in your implementation.\n",
    "### Your implementation must calculate the gradient vector manually, you should *not* use automatic differentiation such as provided by Tensorflow / GradientTape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04c5b71c2452638f5c840e067ef5e6ec",
     "grade": false,
     "grade_id": "task1d-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code (~3-4 lines for each)\n",
    "\"\"\"\n",
    "def loss_mse(theta, X, y):\n",
    "    (n, m) = X.shape\n",
    "    # YOUR CODE HERE\n",
    "    pred_y = np.dot(X, theta)\n",
    "    err = pred_y - y\n",
    "    err2 = np.square(err)\n",
    "    loss_val = np.mean(err2)\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    assert loss_val.shape == ()\n",
    "    return loss_val\n",
    "\n",
    "### For this you'll want to go back to the course slides or spend some time figuring out the gradient of MSE \n",
    "### (the loss) with respect to the parameters (i.e., theta which includes the weights vector w and bias b)\n",
    "### Note: asserts are there to help you ensure that things have the right shape. \n",
    "def gradient_mse(theta, X, y):\n",
    "    (n, m) = X.shape\n",
    "\n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "\n",
    "    ### Recall that the gradient of MSE is: 2/n X^T (θ X - y)   (note: θ = theta)\n",
    "    # YOUR CODE HERE\n",
    "    pred_y = np.dot(X, theta)\n",
    "    err = pred_y - y\n",
    "    gradient_vec = (2/n) * np.dot(X.T, err)\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    return gradient_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25f098c61859018ee2fbd2db615cdf97",
     "grade": true,
     "grade_id": "task1d-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) (5, 1)\n",
      "(5, 1) (5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" [ASSERTS] Check 1d completed. \"\"\"\n",
    "\n",
    "assert var_exists('loss_mse') and var_exists('gradient_mse')\n",
    "tmp_X_ = np.random.randint(low=1, high=10, size=(7,5))\n",
    "tmp_y_ = np.ones((7,1))\n",
    "tmp_theta_ = np.zeros((5,1))\n",
    "\n",
    "tmp_loss_val_ = loss_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "assert np.abs(tmp_loss_val_ - 1.0) < epsf\n",
    "\n",
    "tmp_gv_ = gradient_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "print(tmp_gv_.shape, tmp_theta_.shape)\n",
    "assert tmp_gv_.shape == tmp_theta_.shape and np.all(tmp_gv_ < 0)\n",
    "\n",
    "sum_tmp_gv_ = np.sum(tmp_gv_)\n",
    "tmp_X_ = np.random.randint(low=1, high=10, size=(70,5))\n",
    "tmp_y_ = np.ones((70,1))\n",
    "tmp_gv_ = gradient_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "print(tmp_gv_.shape, tmp_theta_.shape)\n",
    "assert tmp_gv_.shape == tmp_theta_.shape and np.all(tmp_gv_ < 0)\n",
    "sum_tmp_gv_2 = np.sum(tmp_gv_)\n",
    "\n",
    "assert np.abs(sum_tmp_gv_2) < 2*np.abs(sum_tmp_gv_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74a67455125448bef61eae2cd79e26eb",
     "grade": false,
     "grade_id": "task2-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 2] (25 points) Training a Linear Regression Model with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11b183cf9353baf144cb09b3eb47cc93",
     "grade": false,
     "grade_id": "task2-plot-provided",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will use the following plotting function to explore the loss during training (you do not need to modify it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91f0c2b7b10401e641f48f4f875294b7",
     "grade": false,
     "grade_id": "task2-plot-provided-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_gd_data(m_dc, xlim=None, ylim=None, plot_var=True, figsize=(9,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    train_loss = m_dc['train_loss']\n",
    "    val_loss = m_dc['val_loss'] \n",
    "    num_epochs = m_dc['last_epoch']+1\n",
    "    x_arr = 1+np.arange(0, train_loss.shape[0])\n",
    "    \n",
    "    plt.plot(x_arr, train_loss[:,0], 'b--', linewidth=3, label='Training')\n",
    "\n",
    "    if plot_var:\n",
    "        train_loss_epoch_mean = np.mean(train_loss, axis=-1)\n",
    "        train_loss_epoch_std = np.std(train_loss, axis=-1)\n",
    "        z = 1.96\n",
    "        lower = np.maximum(0, train_loss_epoch_mean - z * train_loss_epoch_std)\n",
    "        upper = train_loss_epoch_mean + z * train_loss_epoch_std\n",
    "        plt.fill_between(x_arr, lower, upper, color='b', alpha=0.3)\n",
    "    \n",
    "    plt.plot(x_arr, val_loss, 'r:', linewidth=3, label='Validation')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    if xlim is None:\n",
    "        xlim = np.array([1, num_epochs])\n",
    "    plt.xlim(xlim)\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)    \n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "190b72c0c11a387953ac5c99ce98ea7d",
     "grade": false,
     "grade_id": "cell-7752d8c76d1b98b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Now let's train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3850e753469768a9866178f9e19f50b1",
     "grade": false,
     "grade_id": "task2a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2a] (5 points) Fill in the code below to train the model for 50 epochs with a batch size of 1000, a constant lr schedule and lr=0.004. Use zero_init() for initializing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49f326d706e240e606493e56af935710",
     "grade": false,
     "grade_id": "task2a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 711.91, lr: 0.004000000, theta diff (l2-norm): 0.224040715.\n",
      "[Iter 40 (epoch 2)] train loss: 638.37, lr: 0.004000000, theta diff (l2-norm): 2.088179367.\n",
      "[Iter 80 (epoch 5)] train loss: 574.27, lr: 0.004000000, theta diff (l2-norm): 1.214545563.\n",
      "[Iter 120 (epoch 8)] train loss: 562.70, lr: 0.004000000, theta diff (l2-norm): 0.690172873.\n",
      "[Iter 160 (epoch 11)] train loss: 569.05, lr: 0.004000000, theta diff (l2-norm): 0.395073481.\n",
      "[Iter 200 (epoch 14)] train loss: 588.94, lr: 0.004000000, theta diff (l2-norm): 0.201701523.\n",
      "[Iter 240 (epoch 17)] train loss: 621.26, lr: 0.004000000, theta diff (l2-norm): 0.087454663.\n",
      "[Iter 280 (epoch 20)] train loss: 621.66, lr: 0.004000000, theta diff (l2-norm): 0.022935595.\n",
      "[Iter 320 (epoch 22)] train loss: 604.53, lr: 0.004000000, theta diff (l2-norm): 0.237491663.\n",
      "[Iter 360 (epoch 25)] train loss: 604.05, lr: 0.004000000, theta diff (l2-norm): 0.156951137.\n",
      "[Iter 400 (epoch 28)] train loss: 604.44, lr: 0.004000000, theta diff (l2-norm): 0.098790145.\n",
      "[Iter 440 (epoch 31)] train loss: 629.53, lr: 0.004000000, theta diff (l2-norm): 0.060507400.\n",
      "[Iter 480 (epoch 34)] train loss: 616.33, lr: 0.004000000, theta diff (l2-norm): 0.035978497.\n",
      "[Iter 520 (epoch 37)] train loss: 633.17, lr: 0.004000000, theta diff (l2-norm): 0.017929194.\n",
      "[Iter 560 (epoch 40)] train loss: 654.36, lr: 0.004000000, theta diff (l2-norm): 0.007538143.\n",
      "[Iter 600 (epoch 42)] train loss: 649.13, lr: 0.004000000, theta diff (l2-norm): 0.051104482.\n",
      "[Iter 640 (epoch 45)] train loss: 685.59, lr: 0.004000000, theta diff (l2-norm): 0.036822426.\n",
      "[Iter 680 (epoch 48)] train loss: 665.61, lr: 0.004000000, theta diff (l2-norm): 0.027232255.\n",
      "\n",
      "LR Model w/o Gradient Descent\n",
      "[LinearRegression] Train R^2: 0.890, Val  R^2: 0.891\n",
      "[LinearRegression] Train MSE: 38.227, Val MSE: 38.544\n",
      "[LinearRegression] Train MAE: 3.491, Val MAE: 3.469\n",
      "\n",
      "LR Model w/ Gradient Descent\n",
      "Train R^2: 0.890, Val  R^2: 0.890\n",
      "Train MSE: 38.403, Val MSE: 38.616\n",
      "Train MAE: 3.530, Val MAE: 3.497\n"
     ]
    }
   ],
   "source": [
    "# add a constant feature of 1 to each row to account for the bias term\n",
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "model_fns_tup = (predict_linear, loss_mse, gradient_mse) # set our functions\n",
    "\n",
    "\"\"\"Fill in your code (1 line)\n",
    "\"\"\"\n",
    "# set 'init_params_fn'\n",
    "# YOUR CODE HERE\n",
    "init_params_fn = zero_init\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# use a lambda to define the constant schedule with the learning rate baked in\n",
    "learning_rate = 0.004\n",
    "lr_sched_fn = lambda i: constant_lr_schedule(learning_rate, i)\n",
    "\n",
    "# actually run the gradient descent and store the result\n",
    "batch_size = 1000\n",
    "model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=50, batch_size=batch_size, \n",
    "                               verbose=True, verb_freq=np.maximum(1, 40000//batch_size))\n",
    "print()\n",
    "\n",
    "\"\"\"Fill in your code (1 line)\n",
    "\"\"\"\n",
    "# Use r2_mse_mae_eval to evaluate the model\n",
    "# YOUR CODE HERE\n",
    "def eval_no_sgd(model, tr_x, tr_y, v_x, v_y, pref='', verb=True):\n",
    "    train_r2 = r2_score(tr_y, model.predict(tr_x))\n",
    "    val_r2 = r2_score(v_y, model.predict(v_x))\n",
    "    \n",
    "    if verb:\n",
    "        print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, train_r2, val_r2))\n",
    "\n",
    "    train_pred = model.predict(tr_x)\n",
    "    val_pred = model.predict(v_x)\n",
    "\n",
    "    train_mse = mean_squared_error(tr_y, train_pred)\n",
    "    val_mse = mean_squared_error(v_y, val_pred)\n",
    "\n",
    "    if verb:\n",
    "        print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_mse, val_mse))\n",
    "\n",
    "    train_mae = mean_absolute_error(tr_y, train_pred)\n",
    "    val_mae = mean_absolute_error(v_y, val_pred)\n",
    "\n",
    "    if verb:\n",
    "        print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_mae, val_mae))\n",
    "\n",
    "    return train_r2, val_r2, train_mse, val_mse, train_mae, val_mae\n",
    "\n",
    "print(\"LR Model w/o Gradient Descent\")\n",
    "_ = eval_no_sgd(lrmodel, train_x, train_y, val_x, val_y, pref='[{}] '.format(lrmodel.__class__.__name__))\n",
    "print()\n",
    "print(\"LR Model w/ Gradient Descent\")\n",
    "_ = r2_mse_mae_eval(model_dc['theta'], train_x, train_y, val_x, val_y, predict_fn=predict_linear, pref='', verb=True)\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Model Coefficients (w/o SGD):\n",
      " [ 6.65072209e-01  5.13065833e-01 -2.65445107e-01  1.15211003e+00\n",
      " -1.99749773e-01  2.64843504e-01 -1.63633348e+00 -5.24806304e-01\n",
      "  6.76389178e-03 -1.75097150e-03 -1.33696597e-01  2.16990680e-01\n",
      "  1.69083363e+01 -1.90213155e-01]\n",
      "LR Model Intercept (w/o SGD):\n",
      " 19.323358975980494\n",
      "\n",
      "LR Model Weights (w/ SGD):\n",
      " [ 1.92587387e+01  7.69183433e-01  7.16703750e-01 -2.59405932e-01\n",
      "  1.44358588e+00 -1.82534319e-01  2.70359459e-01 -1.51069490e+00\n",
      " -6.10841684e-01  5.28297065e-03 -5.08357355e-03 -1.10391714e-01\n",
      "  2.53301493e-01  1.64981774e+01 -1.94083502e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"LR Model Coefficients (w/o SGD):\\n\", lrmodel.coef_)\n",
    "print(\"LR Model Intercept (w/o SGD):\\n\", lrmodel.intercept_)\n",
    "print()\n",
    "print(\"LR Model Weights (w/ SGD):\\n\", model_dc['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b470baf4455cb4d70e26035b3f873fd",
     "grade": true,
     "grade_id": "task2a-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2a completed. \"\"\"\n",
    "\n",
    "assert var_exists('init_params_fn') and init_params_fn == zero_init\n",
    "assert model_dc['mb_X_shape'] == (903,15)\n",
    "assert model_dc['theta'].shape == (15,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4621137470321e759bc8f8ccc8b3aee1",
     "grade": false,
     "grade_id": "task2b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2b] (10 points) Answer the following questions (a sentence or two in a comment is fine).\n",
    "### 1. How does the model trained in 2a compare to the linear regression model 'lrmodel' trained at the start of Task 1 in terms of *performance metrics*?\n",
    "### 2. How does the model trained in 2a compare to the linear regression model 'lrmodel' trained at the start of Task 1 in terms of *parameters*?\n",
    "### 3. We used zero init for the initial theta values. A concern with this might be that if theta is 0 the initial gradient could be 0 and so we could get stuck unable to update parameters. Did this happen? Why or why not? (Explain your answer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c4248f05040d327581c4e8fce2c49e8",
     "grade": true,
     "grade_id": "task2b-answer",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## 1. Answer: \n",
    "# The model trained in 2a compared to the linear regression model of Task 1 achieves similar performance metrics. The two are virtually identical although\n",
    "# the model in 2a with gradient descent has very slightly higher MSE and MAE values.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "#\n",
    "## 2. Answer: \n",
    "# The model in 2a has higher magnitudes for coefficients meaning the gradient descent method pushed the weights to better fit the data. The intercepts of the\n",
    "# two models are not that different, but still notably illustrate the difference gradient descent has on a model as its intercept is smaller than that of the\n",
    "# model from Task 1.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "#\n",
    "## 3. Answer: \n",
    "# We can see that the training loss fluctuates meaning the model is trying to fit the data better. Theta's values also fluctuate to different values\n",
    "# through the iterations meaning that the model is continously updating and theta is never getting stuck at zero. So, in this instance, initializing the\n",
    "# parameters with zero did not cause the model to be stuck and unable to update parameters.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64b0491aca504417757c1a3246ff57fa",
     "grade": false,
     "grade_id": "task2c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2c] (5 points) You will now train the model with batch gradient descent (batch size = training set size) and stochastic gradient descent (batch size = 1). In each case you will train the model for 50 epochs and you will need to tune the learning rate appropriately (using a constant lr schedule) to ensure the training process is stable and the resulting model achieves similar performance as the one trained in Task 2a.  To observe the training process, you may find it useful to visualize training using plot_gd_data().\n",
    "## Once you have sufficiently explored the relationship between the learning rate and behavior of training, set the variables below to their appropriate values.\n",
    "### - 'bgd_lr': set this to the value of the learning rate you found for batch gradient descent\n",
    "### - 'sgd_lr': set this to the value of the learning rate you found for stochastic gradient descent\n",
    "### - 'lr_relationship_with_batch_size': +1 if the learning rate should increase with larger batch size, -1 if it should decrease with larger batch size, 0 if lr and batch size are independent/unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02bfbe1b13fb184651f440472c2ba208",
     "grade": false,
     "grade_id": "task2c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 2.36, lr: 0.000010000, theta diff (l2-norm): 0.000103844.\n",
      "[Iter 40000 (epoch 2)] train loss: 6.38, lr: 0.000010000, theta diff (l2-norm): 3.117055463.\n",
      "[Iter 80000 (epoch 5)] train loss: 3.97, lr: 0.000010000, theta diff (l2-norm): 1.149249009.\n",
      "[Iter 120000 (epoch 8)] train loss: 2.64, lr: 0.000010000, theta diff (l2-norm): 0.477032390.\n",
      "[Iter 160000 (epoch 11)] train loss: 4.86, lr: 0.000010000, theta diff (l2-norm): 0.202621000.\n",
      "[Iter 200000 (epoch 14)] train loss: 2.50, lr: 0.000010000, theta diff (l2-norm): 0.094528868.\n",
      "[Iter 240000 (epoch 17)] train loss: 4.68, lr: 0.000010000, theta diff (l2-norm): 0.044794985.\n",
      "[Iter 280000 (epoch 20)] train loss: 2.38, lr: 0.000010000, theta diff (l2-norm): 0.015539083.\n",
      "[Iter 320000 (epoch 23)] train loss: 2.51, lr: 0.000010000, theta diff (l2-norm): 0.006799055.\n",
      "[Iter 360000 (epoch 25)] train loss: 93.88, lr: 0.000010000, theta diff (l2-norm): 0.041634052.\n",
      "[Iter 400000 (epoch 28)] train loss: 47.03, lr: 0.000010000, theta diff (l2-norm): 0.023083795.\n",
      "[Iter 440000 (epoch 31)] train loss: 24.32, lr: 0.000010000, theta diff (l2-norm): 0.021032948.\n",
      "[Iter 480000 (epoch 34)] train loss: 12.32, lr: 0.000010000, theta diff (l2-norm): 0.012297600.\n",
      "[Iter 520000 (epoch 37)] train loss: 9.10, lr: 0.000010000, theta diff (l2-norm): 0.026313874.\n",
      "[Iter 560000 (epoch 40)] train loss: 18.97, lr: 0.000010000, theta diff (l2-norm): 0.015397451.\n",
      "[Iter 600000 (epoch 43)] train loss: 16.82, lr: 0.000010000, theta diff (l2-norm): 0.015266769.\n",
      "[Iter 640000 (epoch 46)] train loss: 86.91, lr: 0.000010000, theta diff (l2-norm): 0.008801455.\n",
      "[Iter 680000 (epoch 48)] train loss: 43.97, lr: 0.000010000, theta diff (l2-norm): 0.017044212.\n",
      "[Iter 0 (epoch 0)] train loss: 659.83, lr: 0.050000000, theta diff (l2-norm): 2.809644995.\n",
      "[Iter 2 (epoch 2)] train loss: 623.94, lr: 0.050000000, theta diff (l2-norm): 2.158911644.\n",
      "[Iter 4 (epoch 4)] train loss: 592.58, lr: 0.050000000, theta diff (l2-norm): 1.680068384.\n",
      "[Iter 6 (epoch 6)] train loss: 574.24, lr: 0.050000000, theta diff (l2-norm): 1.322859060.\n",
      "[Iter 8 (epoch 8)] train loss: 567.29, lr: 0.050000000, theta diff (l2-norm): 1.052831150.\n",
      "[Iter 10 (epoch 10)] train loss: 568.00, lr: 0.050000000, theta diff (l2-norm): 0.846154362.\n",
      "[Iter 12 (epoch 12)] train loss: 573.10, lr: 0.050000000, theta diff (l2-norm): 0.686159730.\n",
      "[Iter 14 (epoch 14)] train loss: 580.30, lr: 0.050000000, theta diff (l2-norm): 0.561033386.\n",
      "[Iter 16 (epoch 16)] train loss: 588.17, lr: 0.050000000, theta diff (l2-norm): 0.462281037.\n",
      "[Iter 18 (epoch 18)] train loss: 595.92, lr: 0.050000000, theta diff (l2-norm): 0.383703864.\n",
      "[Iter 20 (epoch 20)] train loss: 603.13, lr: 0.050000000, theta diff (l2-norm): 0.320712260.\n",
      "[Iter 22 (epoch 22)] train loss: 609.64, lr: 0.050000000, theta diff (l2-norm): 0.269862242.\n",
      "[Iter 24 (epoch 24)] train loss: 615.42, lr: 0.050000000, theta diff (l2-norm): 0.228538761.\n",
      "[Iter 26 (epoch 26)] train loss: 620.50, lr: 0.050000000, theta diff (l2-norm): 0.194736280.\n",
      "[Iter 28 (epoch 28)] train loss: 624.95, lr: 0.050000000, theta diff (l2-norm): 0.166904233.\n",
      "[Iter 30 (epoch 30)] train loss: 628.84, lr: 0.050000000, theta diff (l2-norm): 0.143836183.\n",
      "[Iter 32 (epoch 32)] train loss: 632.24, lr: 0.050000000, theta diff (l2-norm): 0.124588848.\n",
      "[Iter 34 (epoch 34)] train loss: 635.22, lr: 0.050000000, theta diff (l2-norm): 0.108421869.\n",
      "[Iter 36 (epoch 36)] train loss: 637.84, lr: 0.050000000, theta diff (l2-norm): 0.094752297.\n",
      "[Iter 38 (epoch 38)] train loss: 640.14, lr: 0.050000000, theta diff (l2-norm): 0.083119753.\n",
      "[Iter 40 (epoch 40)] train loss: 642.17, lr: 0.050000000, theta diff (l2-norm): 0.073159513.\n",
      "[Iter 42 (epoch 42)] train loss: 643.97, lr: 0.050000000, theta diff (l2-norm): 0.064581592.\n",
      "[Iter 44 (epoch 44)] train loss: 645.56, lr: 0.050000000, theta diff (l2-norm): 0.057154437.\n",
      "[Iter 46 (epoch 46)] train loss: 646.98, lr: 0.050000000, theta diff (l2-norm): 0.050692220.\n",
      "[Iter 48 (epoch 48)] train loss: 648.23, lr: 0.050000000, theta diff (l2-norm): 0.045044928.\n",
      "SGD\n",
      "Train R^2: 0.890, Val  R^2: 0.891\n",
      "Train MSE: 38.227, Val MSE: 38.534\n",
      "Train MAE: 3.493, Val MAE: 3.470\n",
      "\n",
      "Batch SGD\n",
      "Train R^2: 0.889, Val  R^2: 0.890\n",
      "Train MSE: 38.521, Val MSE: 38.712\n",
      "Train MAE: 3.546, Val MAE: 3.511\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "model_fns_tup = (predict_linear, loss_mse, gradient_mse) \n",
    "\n",
    "sgd_lr = 0.00001\n",
    "bgd_lr = 0.05\n",
    "lr_relationship_with_batch_size = 1\n",
    "\n",
    "\"\"\"Fill in your code\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "sgd_batch_size = 1\n",
    "bgd_batch_size = 13903\n",
    "\n",
    "sgd_lr_sched_fn = lambda i: constant_lr_schedule(sgd_lr, i)\n",
    "bgd_lr_sched_fn = lambda i: constant_lr_schedule(bgd_lr, i)\n",
    "\n",
    "sgd_model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, sgd_lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=50, batch_size=sgd_batch_size, \n",
    "                               verbose=True, verb_freq=np.maximum(1, 40000//sgd_batch_size))\n",
    "\n",
    "bgd_model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, bgd_lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=50, batch_size=bgd_batch_size, \n",
    "                               verbose=True, verb_freq=np.maximum(1, 40000//bgd_batch_size))\n",
    "\n",
    "print(\"SGD\")\n",
    "_ = r2_mse_mae_eval(sgd_model_dc['theta'], train_x, train_y, val_x, val_y, predict_fn=predict_linear, pref='', verb=True)\n",
    "print()\n",
    "print(\"Batch SGD\")\n",
    "_ = r2_mse_mae_eval(bgd_model_dc['theta'], train_x, train_y, val_x, val_y, predict_fn=predict_linear, pref='', verb=True)\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96edef268396675c4d2f9f947dc549b2",
     "grade": true,
     "grade_id": "task2c-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2c completed. \"\"\"\n",
    "\n",
    "assert var_exists('sgd_lr') and sgd_lr >= 0\n",
    "assert var_exists('sgd_model_dc') and type(sgd_model_dc) == dict and sgd_model_dc['num_batches'] == train_x.shape[0] and sgd_model_dc['last_epoch'] == 49 \n",
    "assert var_exists('bgd_lr') and bgd_lr >= 0\n",
    "assert var_exists('bgd_model_dc') and type(bgd_model_dc) == dict and bgd_model_dc['num_batches'] == 1 and bgd_model_dc['last_epoch'] == 49 \n",
    "\n",
    "assert var_exists('lr_relationship_with_batch_size') and lr_relationship_with_batch_size in [-1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db7384b9147da7aad4f1632bf6f1470e",
     "grade": false,
     "grade_id": "task2d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2d] (5 points) Now go back to generic_partition_shuffle_sgd and implement gradient clipping by putting your code in the space provided. Ensure that when clip_gradient=True you normalize the gradient vector to have unit L2 norm. Now train the model again for 50 epochs with stochastic gradient descent (batch size = 1) but this time using random init and log1p_decay_lr_schedule. Tune the learning rate appropriately to ensure the training process is stable and the resulting model achieves similar performance as before. Set the 'clipped_grad_sgd_lr' variable to your chosen learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ba035afe4a86bd0d9536d2f6e24f88",
     "grade": false,
     "grade_id": "task2d-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 195.65, lr: 0.300000000, theta diff (l2-norm): 0.300000000.\n",
      "[Iter 40000 (epoch 2)] train loss: 97.85, lr: 0.142951607, theta diff (l2-norm): 1.556676580.\n",
      "[Iter 80000 (epoch 5)] train loss: 48.93, lr: 0.107459114, theta diff (l2-norm): 1.400678471.\n",
      "[Iter 120000 (epoch 8)] train loss: 37.41, lr: 0.093831382, theta diff (l2-norm): 0.693761300.\n",
      "[Iter 160000 (epoch 11)] train loss: 477.75, lr: 0.086085520, theta diff (l2-norm): 1.054309335.\n",
      "[Iter 200000 (epoch 14)] train loss: 238.99, lr: 0.080905054, theta diff (l2-norm): 0.879233338.\n",
      "[Iter 240000 (epoch 17)] train loss: 119.66, lr: 0.077113453, theta diff (l2-norm): 0.937403960.\n",
      "[Iter 280000 (epoch 20)] train loss: 59.92, lr: 0.074174394, theta diff (l2-norm): 0.785238480.\n",
      "[Iter 320000 (epoch 23)] train loss: 29.97, lr: 0.071803766, theta diff (l2-norm): 0.673078545.\n",
      "[Iter 360000 (epoch 25)] train loss: 15.08, lr: 0.070454016, theta diff (l2-norm): 0.812504552.\n",
      "[Iter 400000 (epoch 28)] train loss: 7.73, lr: 0.068692393, theta diff (l2-norm): 0.898171918.\n",
      "[Iter 440000 (epoch 31)] train loss: 84.18, lr: 0.067178178, theta diff (l2-norm): 0.686396075.\n",
      "[Iter 480000 (epoch 34)] train loss: 42.09, lr: 0.065856658, theta diff (l2-norm): 0.826958930.\n",
      "[Iter 520000 (epoch 37)] train loss: 21.46, lr: 0.064688825, theta diff (l2-norm): 0.875610082.\n",
      "[Iter 560000 (epoch 40)] train loss: 12.10, lr: 0.063645998, theta diff (l2-norm): 0.612196927.\n",
      "[Iter 600000 (epoch 43)] train loss: 6.05, lr: 0.062706544, theta diff (l2-norm): 0.613115241.\n",
      "[Iter 640000 (epoch 46)] train loss: 5.12, lr: 0.061853788, theta diff (l2-norm): 0.509663982.\n",
      "[Iter 680000 (epoch 48)] train loss: 2.57, lr: 0.061326864, theta diff (l2-norm): 0.972534907.\n",
      "Clipped SGD\n",
      "Train R^2: 0.877, Val  R^2: 0.877\n",
      "Train MSE: 42.950, Val MSE: 43.439\n",
      "Train MAE: 3.145, Val MAE: 3.172\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 1\n",
    "\n",
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "model_fns_tup = (predict_linear, loss_mse, gradient_mse) \n",
    "\n",
    "clipped_grad_sgd_lr = 0.3\n",
    "init_params_fn = random_uniform_init\n",
    "clipped_lr_sched_fn = lambda i: log1p_decay_lr_schedule(clipped_grad_sgd_lr, i)\n",
    "\n",
    "\"\"\"Fill in your code\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "clipped_model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, clipped_lr_sched_fn, val_data=(val_x_with_b, val_y), clip_gradient=True,num_epochs=num_epochs, batch_size=batch_size, \n",
    "                               verbose=True, verb_freq=np.maximum(1, 40000//batch_size))\n",
    "\n",
    "print(\"Clipped SGD\")\n",
    "_ = r2_mse_mae_eval(clipped_model_dc['theta'], train_x, train_y, val_x, val_y, predict_fn=predict_linear, pref='', verb=True)\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68cf107510a9ac7c76a69dfbb31437cb",
     "grade": true,
     "grade_id": "task2d-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2d completed. \"\"\"\n",
    "\n",
    "assert var_exists('clipped_grad_sgd_lr') and clipped_grad_sgd_lr >= 0\n",
    "assert var_exists('clipped_model_dc') and type(clipped_model_dc) == dict and clipped_model_dc['num_batches'] == train_x.shape[0] and clipped_model_dc['last_epoch'] == 49 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cca3ffd52efa4a5be9f2432f2c10e39",
     "grade": false,
     "grade_id": "task3-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 3] (25 points) A Linear Regression Model with Pseudo-Huber Loss?\n",
    "## In this task you will do gradient descent with a regularized Pseudo-Huber loss. Before proceeding with the implementation you may want to read up on the Huber loss and the Pseudo-Huber loss.\n",
    "## For our purposes the Pseudo-Huber loss is given by: $$L_{\\bf \\theta}({\\bf{x}},{\\bf{y}}) = \\delta^2 \\sqrt{1 + \\frac{1}{\\delta^2}({\\bf y - \\theta \\cdot x})^2} - \\delta^2 \\ ,$$ where $\\delta > 0$ is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b90bf6cbc240d0519b241ade2e9273f5",
     "grade": false,
     "grade_id": "task3a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 3a] (10 points) First implement the pseudo-huber loss in loss_pseudohuber(). The function should return the loss value averaged across all examples. Then spend time to figure out the gradient of the loss with respect to theta. Once you have it, implement it in gradient_pseudohuber(). Here again the gradient should be computed as the average across all examples. For now you should ignore the argument 'reg_fn' which can be used to add a regularization penalty (as this functionality is already implemented in gradient_pseudohuber()). \n",
    "### *[Important]* Your implementation must calculate the gradient vector manually, you should *not* use automatic differentiation such as provided by Tensorflow / GradientTape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6b3285fd37792ab18a55dc10e17809",
     "grade": false,
     "grade_id": "task3a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code here.\n",
    "\"\"\"\n",
    "def loss_pseudohuber(theta, X, y, delta):\n",
    "    (n, m) = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "\n",
    "    ### Calculate loss_val for the pseudo-huber loss (scalar)\n",
    "    # YOUR CODE HERE\n",
    "    error = y - np.dot(X, theta)\n",
    "    err2 = error ** 2\n",
    "    loss_val = np.mean(delta ** 2 * (np.sqrt(1 + (err2 / delta **2)) - 1))\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    assert loss_val.shape == ()\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "def gradient_pseudohuber(theta, X, y, delta, reg_fn=None):\n",
    "    (n, m) = X.shape\n",
    "    \n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "    \n",
    "    ### Figure out the gradient for pseudo-huber loss  and implement this function \n",
    "    ### Compute 'gradient_vec' for the pseudo-huber loss ((m,1) vector) \n",
    "    ###* put your code here (~3-5 lines) *###\n",
    "    # YOUR CODE HERE\n",
    "    error = y - np.dot(X, theta)\n",
    "    err2 = error ** 2\n",
    "    \n",
    "    gradient_vec = np.dot(X.T, np.where(err2 <= delta ** 2,\n",
    "                                        -error * (1 / np.sqrt(1 + err2 / delta**2)),\n",
    "                                        -delta * np.sign(error)))\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    if reg_fn != None:\n",
    "        reg_grad = reg_fn(theta)\n",
    "        assert gradient_vec.shape == reg_grad.shape\n",
    "        gradient_vec += reg_grad\n",
    "\n",
    "    return gradient_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8b29db6b86787a3d873333ca312a190",
     "grade": true,
     "grade_id": "task3a-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) (5, 1)\n",
      "(5, 1) (5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" [ASSERTS] Check 3a completed. \"\"\"\n",
    "\n",
    "assert var_exists('loss_pseudohuber')\n",
    "assert var_exists('gradient_pseudohuber')\n",
    "\n",
    "tmp_X_ = np.random.randint(low=1, high=10, size=(7,5))\n",
    "tmp_y_ = np.ones((7,1)) * 1\n",
    "tmp_theta_ = np.zeros((5,1))\n",
    "\n",
    "pseudohuber_delta = 0.05\n",
    "\n",
    "tmp_loss_val_ = loss_pseudohuber(tmp_theta_, tmp_X_, tmp_y_, pseudohuber_delta)\n",
    "tmp_loss_val_mse_ = loss_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "assert tmp_loss_val_ >= 0 and tmp_loss_val_ < tmp_loss_val_mse_ and tmp_loss_val_  < pseudohuber_delta + epsf \n",
    "\n",
    "pseudohuber_delta = 1.0\n",
    "tmp_y_ = np.ones((7,1)) * 0.1\n",
    "tmp_loss_val_mse_ = loss_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "tmp_loss_val_ = loss_pseudohuber(tmp_theta_, tmp_X_, tmp_y_, pseudohuber_delta)\n",
    "assert tmp_loss_val_ >= 0 and tmp_loss_val_ < tmp_loss_val_mse_\n",
    "\n",
    "tmp_gv_ = gradient_pseudohuber(tmp_theta_, tmp_X_, tmp_y_, pseudohuber_delta)\n",
    "print(tmp_gv_.shape, tmp_theta_.shape)\n",
    "assert tmp_gv_.shape == tmp_theta_.shape and np.all(tmp_gv_ < 0)\n",
    "\n",
    "tmp_gv_mse_ = gradient_mse(tmp_theta_, tmp_X_, tmp_y_)\n",
    "print(tmp_gv_mse_.shape, tmp_theta_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c98db46d01d66f3b0af8d59a419f63ba",
     "grade": false,
     "grade_id": "task3a-helper1-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following can be used to test your implementation of the pseudo-huber loss and gradient. (You do not need to modify it.) Training should converge to a model comparable in performance to the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "665e307a647bbb372084c3f064e289c3",
     "grade": false,
     "grade_id": "task3a-helper-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 4.92, lr: 0.002000000, theta diff (l2-norm): 0.484594147.\n",
      "\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.672, Val MSE: 43.404\n",
      "Train MAE: 3.121, Val MAE: 3.158\n"
     ]
    }
   ],
   "source": [
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "delta = 0.25\n",
    "\n",
    "loss_ph = lambda _t, _x, _y: loss_pseudohuber(_t, _x, _y, delta)\n",
    "grph = lambda _t, _x, _y: gradient_pseudohuber(_t, _x, _y, delta, None)\n",
    "\n",
    "# set our functions\n",
    "model_fns_tup = (predict_linear, loss_ph, grph)\n",
    "init_params_fn = random_uniform_init\n",
    "\n",
    "learning_rate = 0.002 \n",
    "lr_sched_fn = lambda i: log1p_decay_lr_schedule(learning_rate, i)\n",
    "\n",
    "# run the gradient descent and store the result\n",
    "model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=50, batch_size=1000, \n",
    "                               clip_gradient=False, verbose=True, verb_freq=np.maximum(1, 100000//batch_size))\n",
    "\n",
    "print()\n",
    "_ = r2_mse_mae_eval(model_dc['theta'], train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8533a1a5bfa80ded77d36778c64f10d8",
     "grade": false,
     "grade_id": "task3b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 3b] (10 points) Now you will implement L2 regularization and use it to train a model with the pseudo-huber loss. Fill in the code in l2_regularize() to regularize 'w' (but not the bias/intercept 'b')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15c9b1688fad1ba3b40839d680a04afa",
     "grade": false,
     "grade_id": "task3b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code (~1-2 lines). This function will return the gradient of the L2 regularization term with regularization strength given by 'lmbda'.\n",
    "\"\"\"\n",
    "def l2_regularize(theta, lmbda):\n",
    "    assert lmbda >= 0.0\n",
    "    \n",
    "    ### Note 1: use 'lmbda' (lambda) -- the regularization hyperparameter.\n",
    "    ### Note 2: we do not regularize the bias/intercept b\n",
    "    # YOUR CODE HERE\n",
    "    reg_grad = lmbda * theta\n",
    "    reg_grad[0] = 0\n",
    "    # raise NotImplementedError()\n",
    "    assert reg_grad.shape == theta.shape\n",
    "    \n",
    "    return reg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65ade6ffdb000ec24ffd50bd242e8192",
     "grade": true,
     "grade_id": "task3b-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3b completed. \"\"\"\n",
    "\n",
    "assert var_exists('l2_regularize')\n",
    "\n",
    "tmp_theta_ = np.zeros((5,1))\n",
    "assert np.all(np.abs(l2_regularize(tmp_theta_, 1.0)) < epsf)\n",
    "\n",
    "tmp_theta_ = np.ones((5,1))\n",
    "assert np.sum(np.abs(l2_regularize(tmp_theta_, 1.0))) < np.sum(np.abs(l2_regularize(tmp_theta_, 2.0)))\n",
    "\n",
    "tmp_theta1_ = np.ones((5,1))\n",
    "tmp_theta2_ = np.ones((5,1)) * 2.0\n",
    "assert np.sum(np.abs(l2_regularize(tmp_theta1_, 1.0))) < np.sum(np.abs(l2_regularize(tmp_theta2_, 1.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21419214fecac6847de690675b200298",
     "grade": false,
     "grade_id": "task3c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 3c] (5 points) Train the model to convergence with various strengths of regularization and each time print the performance metrics and the L2 norm of the 'w' to show the effectiveness of regularization. Use the provided learning rate, schedule, param initialization, etc. Finally set the variable 'best_lmbda' to indicate which value of the regularization hyperparamneter lambda is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e59913c9a3757fc46b63fb77a01c955",
     "grade": false,
     "grade_id": "task3c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 4.23, lr: 0.002000000, theta diff (l2-norm): 0.420630325.\n",
      "[Iter 100 (epoch 7)] train loss: 2.93, lr: 0.000649468, theta diff (l2-norm): 0.278678576.\n",
      "[Iter 200 (epoch 14)] train loss: 1.88, lr: 0.000539367, theta diff (l2-norm): 0.201162286.\n",
      "[Iter 300 (epoch 21)] train loss: 1.34, lr: 0.000488873, theta diff (l2-norm): 0.054701126.\n",
      "[Iter 400 (epoch 28)] train loss: 1.02, lr: 0.000457949, theta diff (l2-norm): 0.030545483.\n",
      "[Iter 500 (epoch 35)] train loss: 0.88, lr: 0.000436346, theta diff (l2-norm): 0.025726240.\n",
      "[Iter 600 (epoch 42)] train loss: 0.76, lr: 0.000420062, theta diff (l2-norm): 0.024565964.\n",
      "[i: 0] -- Lambda: 0.000, L2-norm of w: 17.055\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.663, Val MSE: 43.428\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 4.73, lr: 0.002000000, theta diff (l2-norm): 0.484905380.\n",
      "[Iter 100 (epoch 7)] train loss: 3.20, lr: 0.000649468, theta diff (l2-norm): 0.273249477.\n",
      "[Iter 200 (epoch 14)] train loss: 2.02, lr: 0.000539367, theta diff (l2-norm): 0.254682697.\n",
      "[Iter 300 (epoch 21)] train loss: 1.37, lr: 0.000488873, theta diff (l2-norm): 0.061113198.\n",
      "[Iter 400 (epoch 28)] train loss: 1.07, lr: 0.000457949, theta diff (l2-norm): 0.026475558.\n",
      "[Iter 500 (epoch 35)] train loss: 0.89, lr: 0.000436346, theta diff (l2-norm): 0.027893439.\n",
      "[Iter 600 (epoch 42)] train loss: 0.79, lr: 0.000420062, theta diff (l2-norm): 0.020879525.\n",
      "[i: 1] -- Lambda: 0.010, L2-norm of w: 17.057\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.642, Val MSE: 43.404\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 5.10, lr: 0.002000000, theta diff (l2-norm): 0.482770953.\n",
      "[Iter 100 (epoch 7)] train loss: 3.34, lr: 0.000649468, theta diff (l2-norm): 0.266259274.\n",
      "[Iter 200 (epoch 14)] train loss: 2.07, lr: 0.000539367, theta diff (l2-norm): 0.252412448.\n",
      "[Iter 300 (epoch 21)] train loss: 1.42, lr: 0.000488873, theta diff (l2-norm): 0.070338528.\n",
      "[Iter 400 (epoch 28)] train loss: 1.10, lr: 0.000457949, theta diff (l2-norm): 0.023005148.\n",
      "[Iter 500 (epoch 35)] train loss: 0.91, lr: 0.000436346, theta diff (l2-norm): 0.018370962.\n",
      "[Iter 600 (epoch 42)] train loss: 0.83, lr: 0.000420062, theta diff (l2-norm): 0.019014191.\n",
      "[i: 2] -- Lambda: 0.100, L2-norm of w: 17.057\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.636, Val MSE: 43.403\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 4.44, lr: 0.002000000, theta diff (l2-norm): 0.469441538.\n",
      "[Iter 100 (epoch 7)] train loss: 3.07, lr: 0.000649468, theta diff (l2-norm): 0.263237864.\n",
      "[Iter 200 (epoch 14)] train loss: 1.96, lr: 0.000539367, theta diff (l2-norm): 0.230725951.\n",
      "[Iter 300 (epoch 21)] train loss: 1.31, lr: 0.000488873, theta diff (l2-norm): 0.062473237.\n",
      "[Iter 400 (epoch 28)] train loss: 1.03, lr: 0.000457949, theta diff (l2-norm): 0.032156933.\n",
      "[Iter 500 (epoch 35)] train loss: 0.89, lr: 0.000436346, theta diff (l2-norm): 0.019495698.\n",
      "[Iter 600 (epoch 42)] train loss: 0.85, lr: 0.000420062, theta diff (l2-norm): 0.022927669.\n",
      "[i: 3] -- Lambda: 1.000, L2-norm of w: 17.057\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.638, Val MSE: 43.407\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 4.73, lr: 0.002000000, theta diff (l2-norm): 0.466170940.\n",
      "[Iter 100 (epoch 7)] train loss: 3.21, lr: 0.000649468, theta diff (l2-norm): 0.271799180.\n",
      "[Iter 200 (epoch 14)] train loss: 1.98, lr: 0.000539367, theta diff (l2-norm): 0.191710011.\n",
      "[Iter 300 (epoch 21)] train loss: 1.32, lr: 0.000488873, theta diff (l2-norm): 0.057280715.\n",
      "[Iter 400 (epoch 28)] train loss: 1.02, lr: 0.000457949, theta diff (l2-norm): 0.021866241.\n",
      "[Iter 500 (epoch 35)] train loss: 0.87, lr: 0.000436346, theta diff (l2-norm): 0.024236247.\n",
      "[Iter 600 (epoch 42)] train loss: 0.77, lr: 0.000420062, theta diff (l2-norm): 0.021794770.\n",
      "[i: 4] -- Lambda: 10.000, L2-norm of w: 17.053\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.640, Val MSE: 43.399\n",
      "Train MAE: 3.120, Val MAE: 3.158\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 4.84, lr: 0.002000000, theta diff (l2-norm): 0.481506296.\n",
      "[Iter 100 (epoch 7)] train loss: 3.21, lr: 0.000649468, theta diff (l2-norm): 0.268184352.\n",
      "[Iter 200 (epoch 14)] train loss: 2.02, lr: 0.000539367, theta diff (l2-norm): 0.230617820.\n",
      "[Iter 300 (epoch 21)] train loss: 1.37, lr: 0.000488873, theta diff (l2-norm): 0.062790694.\n",
      "[Iter 400 (epoch 28)] train loss: 1.06, lr: 0.000457949, theta diff (l2-norm): 0.028533314.\n",
      "[Iter 500 (epoch 35)] train loss: 0.88, lr: 0.000436346, theta diff (l2-norm): 0.023416538.\n",
      "[Iter 600 (epoch 42)] train loss: 0.76, lr: 0.000420062, theta diff (l2-norm): 0.021849557.\n",
      "[i: 5] -- Lambda: 100.000, L2-norm of w: 17.055\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.640, Val MSE: 43.405\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n",
      "[Iter 0 (epoch 0)] train loss: 5.06, lr: 0.002000000, theta diff (l2-norm): 0.480929070.\n",
      "[Iter 100 (epoch 7)] train loss: 3.42, lr: 0.000649468, theta diff (l2-norm): 0.275897768.\n",
      "[Iter 200 (epoch 14)] train loss: 2.18, lr: 0.000539367, theta diff (l2-norm): 0.277836019.\n",
      "[Iter 300 (epoch 21)] train loss: 1.46, lr: 0.000488873, theta diff (l2-norm): 0.085588190.\n",
      "[Iter 400 (epoch 28)] train loss: 1.09, lr: 0.000457949, theta diff (l2-norm): 0.032823073.\n",
      "[Iter 500 (epoch 35)] train loss: 0.94, lr: 0.000436346, theta diff (l2-norm): 0.021935608.\n",
      "[Iter 600 (epoch 42)] train loss: 0.81, lr: 0.000420062, theta diff (l2-norm): 0.022213331.\n",
      "[i: 6] -- Lambda: 1000.000, L2-norm of w: 17.055\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.636, Val MSE: 43.405\n",
      "Train MAE: 3.120, Val MAE: 3.159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "delta = 0.25\n",
    "\n",
    "loss_ph = lambda _t, _x, _y: loss_pseudohuber(_t, _x, _y, delta)\n",
    "init_params_fn = random_uniform_init\n",
    "\n",
    "learning_rate = 0.002 \n",
    "lr_sched_fn = lambda i: log1p_decay_lr_schedule(learning_rate, i)\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 1000\n",
    "\n",
    "best_lmbda = 0.0\n",
    "\n",
    "lmbdas = np.array([0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0])\n",
    "for i, lmbda in enumerate(lmbdas):\n",
    "    reg_fn = lambda _t: l2_regularize(_t, lmbda)\n",
    "    # YOUR CODE HERE\n",
    "    model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=num_epochs, \n",
    "                                             batch_size=batch_size, clip_gradient=False, verbose=True, verb_freq=np.maximum(1, 100000//batch_size))\n",
    "    l2norm_w = np.linalg.norm(model_dc['theta'][1:])\n",
    "    # raise NotImplementedError()\n",
    "    print('[i: {}] -- Lambda: {:.3f}, L2-norm of w: {:.3f}'.format(i, lmbda, l2norm_w))\n",
    "    _ = r2_mse_mae_eval(model_dc['theta'], train_x, train_y, val_x, val_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeb793597648bef238587541dc04ac66",
     "grade": true,
     "grade_id": "task3c-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3c completed. \"\"\"\n",
    "\n",
    "assert var_exists('lmbdas') and np.array_equal(lmbdas, np.array([0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]))\n",
    "assert var_exists('best_lmbda') and best_lmbda != None and best_lmbda >= 0 and best_lmbda < +np.inf and best_lmbda in lmbdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e97a83171b7df534a07fae4b342cbba",
     "grade": false,
     "grade_id": "task4-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 4] (20 points) PCA\n",
    "## In this task you will use PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1265c98a7f6e7a77fb88620d743fadc0",
     "grade": false,
     "grade_id": "task4a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 4a] (10 points) Use PCA to transform both the unscaled ('all_x') and scaled ('scaled_all_x') features. In each case you should reduce the dimensionality to keep only k principal components where k is the *smallest positive* integer that captures at least 90% of the variance. After you transform the data ('all_x_transf' and 'scaled_all_x_transf') be sure to set the variables 'k_90perc_unscaled' and 'k_90perc_scaled'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99eeb90c49e07689cb74ffc61898827d",
     "grade": false,
     "grade_id": "task4a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Unscaled Data (k=1):  98.3%\n",
      "Explained Variance for Scaled Data (k=12):  94.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# YOUR CODE HERE\n",
    "k = 1\n",
    "pca_unscaled = PCA(n_components=k, whiten=True).fit(all_x)\n",
    "all_x_transf = pca_unscaled.transform(all_x)\n",
    "k_unscaled = np.sum(pca_unscaled.explained_variance_ratio_)\n",
    "print('Explained Variance for Unscaled Data (k={}): {: .1f}%'.format(k, k_unscaled*100))\n",
    "\n",
    "k=12\n",
    "pca_scaled = PCA(n_components=k, whiten=True).fit(scaled_all_x)\n",
    "scaled_all_x_transf = pca_scaled.transform(scaled_all_x)\n",
    "k_scaled = np.sum(pca_scaled.explained_variance_ratio_)\n",
    "print('Explained Variance for Scaled Data (k={}): {: .1f}%'.format(k, k_scaled*100))\n",
    "\n",
    "k_90perc_unscaled = 1\n",
    "k_90perc_scaled = 12\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "766bc7055ebf7d2d419fbed0d657c009",
     "grade": true,
     "grade_id": "task4a-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 4a completed. \"\"\"\n",
    "\n",
    "assert var_exists('k_90perc_unscaled') and k_90perc_unscaled >= 1 and k_90perc_unscaled <= all_x.shape[1]\n",
    "assert var_exists('k_90perc_scaled') and k_90perc_scaled >= 1 and k_90perc_scaled <= all_x.shape[1]\n",
    "assert var_exists('all_x_transf') and all_x_transf.shape == (all_x.shape[0],k_90perc_unscaled) \n",
    "assert var_exists('scaled_all_x_transf') and scaled_all_x_transf.shape == (all_x.shape[0],k_90perc_scaled) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a73f9d9524a037b9120c5470ced3c51b",
     "grade": false,
     "grade_id": "task4b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 4b] (5 points) Given this, what do you conclude about features scaling when applying PCA. Justify your answer. (If it helps you can try to train a model on the projected features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac62da2de2076e06eecd7164da553f39",
     "grade": true,
     "grade_id": "task4b-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "# Feature scaling when applying PCA has an effect on performance. With the unscaled features, it only takes 1\n",
    "# principal component to get 98.3% accuracy, but this could make the model biased towards just the first feature.\n",
    "# Even using a value greater than 1 for the unscaled data would simply make the model biased towards features\n",
    "# with the largest variances. Using scaled data requires more principal components in order to realize a similar variance\n",
    "# showing how sensitive PCA is to scaled data. With scaled data, each feature has a chance to contribute more evenly to\n",
    "# discourage bias towards certain features.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b4da9fe5972282b09e9f3d93a56964",
     "grade": false,
     "grade_id": "task5-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 5] \\<*For CAI6108MLE Only*\\> (25 points) Least Absolute Deviation. For this task you will use automatic differentiation through Tensorflow's GradientTape to implement LAD regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbd161fd14a2ac29c5c45918ed272805",
     "grade": false,
     "grade_id": "task5a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 5a] (25 points) First implement the LAD loss (mean absolute error) in loss_lad(). The function should return the loss value averaged across all examples. Then use GradientTape to implement the LAD loss in gradient_lad(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "232fd023fa957c2f9dc3384a0e31bb71",
     "grade": false,
     "grade_id": "task5a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: do *not* import or use tensorflow before this point (i.e., before task 5) or automated tests will fail causing you to lose points!\n",
    "import tensorflow as tf \n",
    "\n",
    "\"\"\"Fill in your code here.\n",
    "\"\"\"\n",
    "def loss_lad(theta, X, y):\n",
    "    (n, m) = X.shape\n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    error = y - tf.matmul(X, theta)\n",
    "    loss_val = tf.reduce_mean(tf.abs(error))\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    assert loss_val.shape == ()\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "def gradient_lad(theta, X, y):\n",
    "    (n, m) = X.shape\n",
    "    \n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "    \n",
    "    # first let's create a tensor for theta\n",
    "    theta_tensor = tf.Variable(theta, name='theta')\n",
    "\n",
    "    ### Use GradientTape to compute the gradient. Hint: use tf.reduce_mean().\n",
    "    ###* put your code here (~3-5 lines) *###\n",
    "    # YOUR CODE HERE\n",
    "    with tf.GradientTape() as tape:\n",
    "        error = y - tf.matmul(X, theta_tensor)\n",
    "        loss_val = tf.reduce_mean(tf.abs(error))\n",
    "        \n",
    "    gradient_vec = tape.gradient(loss_val, theta_tensor)\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    assert isinstance(gradient_vec, tf.Tensor)\n",
    "    gradient_vec = gradient_vec.numpy()\n",
    "    return gradient_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2cf44f687a2c95ba2c6df644a4783bb",
     "grade": true,
     "grade_id": "task5a-checks",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0 (epoch 0)] train loss: 18.71, lr: 0.800000000, theta diff (l2-norm): 0.699083358.\n",
      "[Iter 100 (epoch 7)] train loss: 11.05, lr: 0.259787364, theta diff (l2-norm): 0.249717063.\n",
      "[Iter 200 (epoch 14)] train loss: 7.16, lr: 0.215746809, theta diff (l2-norm): 0.061238174.\n",
      "[Iter 300 (epoch 21)] train loss: 4.94, lr: 0.195549181, theta diff (l2-norm): 0.056581685.\n",
      "[Iter 400 (epoch 28)] train loss: 3.84, lr: 0.183179714, theta diff (l2-norm): 0.035773635.\n",
      "[Iter 500 (epoch 35)] train loss: 3.50, lr: 0.174538386, theta diff (l2-norm): 0.037148792.\n",
      "[Iter 600 (epoch 42)] train loss: 3.35, lr: 0.168024864, theta diff (l2-norm): 0.038097385.\n",
      "\n",
      "Train R^2: 0.878, Val  R^2: 0.877\n",
      "Train MSE: 42.693, Val MSE: 43.468\n",
      "Train MAE: 3.120, Val MAE: 3.159\n"
     ]
    }
   ],
   "source": [
    "\"\"\" [ASSERTS] Check 5a completed. \"\"\"\n",
    "\n",
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "val_x_with_b = np.c_[np.ones((val_x.shape[0],1)), val_x]\n",
    "\n",
    "\n",
    "# set our functions\n",
    "model_fns_tup = (predict_linear, loss_lad, gradient_lad)\n",
    "init_params_fn = random_uniform_init\n",
    "\n",
    "learning_rate = 0.8\n",
    "lr_sched_fn = lambda i: log1p_decay_lr_schedule(learning_rate, i)\n",
    "\n",
    "# run the gradient descent and store the result\n",
    "model_dc = generic_partition_shuffle_sgd(X_with_b, train_y, model_fns_tup, init_params_fn, lr_sched_fn, val_data=(val_x_with_b, val_y), num_epochs=50, batch_size=1000, \n",
    "                               clip_gradient=True, verbose=True, verb_freq=np.maximum(1, 100000//batch_size))\n",
    "\n",
    "print()\n",
    "_ = r2_mse_mae_eval(model_dc['theta'], train_x, train_y, val_x, val_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
